package com.diffbot.ml;

import com.diffbot.toolbox.FileTools;
import com.esotericsoftware.minlog.Log;
import com.google.common.base.Splitter;
import com.google.common.collect.ImmutableSet;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.IOException;
import java.util.*;

/**
 * Java implementation of Path-Constraint Resource Allocation.
 *
 * See Wang et al, Knowledge Graph and Text Jointly Embedding, 2014 and
 * https://github.com/thunlp/KB2E/tree/master/PTransE
 *
 * Required inputs in the input directory (generated by KGCompletion.createKb2eInputFiles):
 * train.txt: training file, format (e1, e2, rel).
 * valid.txt: validation file, same format as train.txt -- currently unused
 * test.txt: test file, same format as train.txt.
 * entity2id.txt: all entities and corresponding ids, one per line.
 * relation2id.txt: all relations and corresponding ids, one per line.
 * e1_e2.txt: all top-500 entity pairs mentioned in the task entity prediction.
 *
 * TODO: reimplement in Spark/Tensorflow (see https://github.com/thunlp/TensorFlow-TransX)
 */
public class Pcra {
    private static final float MIN_RESOURCE = 0.01f;
    private static final Splitter WHITESPACE_SPLITTER = Splitter.onPattern("\\s+").trimResults();
    private static final File TEST_PRA_FILE = new File(KGCompletion.KB2E_DIRECTORY, "test_pra.txt");
    private static final File TRAIN_PRA_FILE = new File(KGCompletion.KB2E_DIRECTORY, "train_pra.txt");

    protected static final File PATH2_FILE = new File(KGCompletion.KB2E_DIRECTORY, "path2.txt");
    protected static final File CONFIDENCE_FILE = new File(KGCompletion.KB2E_DIRECTORY, "confidence.txt");

    protected enum Mode {
        TEST {
            @Override public File getTriplesFile() {
                return KGCompletion.TEST_FILE;
            }

            @Override public File getPraFile() {
                return TEST_PRA_FILE;
            }
        },
        TRAIN {
            @Override public File getTriplesFile() {
                return KGCompletion.TRAIN_FILE;
            }

            @Override public File getPraFile() {
                return TRAIN_PRA_FILE;
            }
        },
        ;

        public abstract File getTriplesFile();
        public abstract File getPraFile();
    }



    /**
     * Attempts to determine the reliability of 2-step relation paths by allocating a fixed budget
     * of `resource` to flow from each relation to its co-occurring relations and maintaining
     * the paths with > MIN_RESOURCE.
     *
     * For example, BornInState->StateInCountry implies Nationality with high reliability but
     * Friend->Profession does not generalize and should have low resource and low reliability.
     *
     * See Lin et al, Modeling Relation Paths for Representation Learning of Knowledge Bases, 2015
     *
     * TODO: parallelize
     */
    public static void pathConstraintResourceAllocation() throws IOException {
        long startMs = System.currentTimeMillis();

        Log.info("PCRA", "Loading relations...");
        Map<String, String> relationToId = getRelationIds();
        int relationCount = relationToId.size();
        Log.info("PCRA", String.format("Loaded %d relations in %dms.",
                relationCount, System.currentTimeMillis() - startMs));

        // map of `headId tailId` -> relationId's -- renamed from `ok`
        Map<String, Set<String>> pairRelationMap = new HashMap<>();
        // map of `headId` -> (map of `relationId` -> tailId's) -- renamed from `a`
        Map<String, Map<String, Set<String>>> headRelationTailMap = new HashMap<>();

        startMs = System.currentTimeMillis();
        Log.info("PCRA", "Loading training relations...");
        int trainingTriples = addTrainingRelations(pairRelationMap, headRelationTailMap,
                relationToId, relationCount);
        Log.info("PCRA", String.format("Loaded %d training relations in %dms.",
                trainingTriples, System.currentTimeMillis() - startMs));

        startMs = System.currentTimeMillis();
        Log.info("PCRA", "Loading eval pairs...");
        int evalPairs = addEvalPairs(pairRelationMap);
        Log.info("PCRA", String.format("Loaded %d eval pairs in %dms.", evalPairs,
                System.currentTimeMillis() - startMs));

        // map of `headId tailId` -> map of (relationId -> resource) -- renamed from `h_e_p`
        Map<String, Map<String, Float>> pathResources = new HashMap<>();
        // counts of relation occurrences -- renamed from `path_dict`
        Map<String, Integer> pathCounts = new HashMap<>();
        // counts of relation co-occurrences (like EMPLOYER->LOCATION) -- renamed from `path_r_dict`
        Map<String, Integer> relatedPathCounts = new HashMap<>();
        // relations with > MIN_RESOURCE -- renamed from `train_path`
        Set<String> reliablePaths = new HashSet<>();

        startMs = System.currentTimeMillis();
        Log.info("PCRA", "Executing Path-Constraint Resource Allocation...");
        int countedHeadEntities = 0;
        int totalPaths = 0;
        try (BufferedWriter path2Writer = FileTools.bufferedWriter(PATH2_FILE)) {
//            System.out.println(headRelationTailMap.size());
            for (String headId : headRelationTailMap.keySet()) {
//            for (String headId : ImmutableList.of("/m/06rf7", "/m/0c94fn", "/m/016ywr")) {
                countedHeadEntities++;
//                System.out.println(headId + "\t" + countedHeadEntities);

                // Count 1-hop paths
                for (String relationId : headRelationTailMap.get(headId).keySet()) {
                    Set<String> tailIds = headRelationTailMap.get(headId).get(relationId);
                    for (String tailId : tailIds) {
                        mapIncrement(pathCounts, relationId);

                        String entityPairKey = getEntityPairKey(headId, tailId);
                        for (String nestedRelationId : pairRelationMap.get(entityPairKey)) {
//                            System.out.println("\t\t" + getPathKey(relationId, nestedRelationId));
                            mapIncrement(relatedPathCounts, getPathKey(relationId, nestedRelationId));
                        }

                        float delta = 1 / (float) tailIds.size();
//                        System.out.println("\t\t" + getEntityPairKey(headId, tailId) + " " + relationId + " " + delta);
                        nestedMapAdd(pathResources, getEntityPairKey(headId, tailId), relationId, delta);
                    }
                }

                // Count 2-hop paths and flow resources from 1-hop parent path
                for (String relationId : headRelationTailMap.get(headId).keySet()) {
                    Set<String> tailIds = headRelationTailMap.get(headId).get(relationId);
                    for (String tailId : tailIds) {
                        String entityPairKey = getEntityPairKey(headId, tailId);

                        // Traverse relations sourced from the tail (2-hops from the original head)
                        if (headRelationTailMap.containsKey(tailId)) {
                            for (String nestedRelationId : headRelationTailMap.get(tailId).keySet()) {
                                Set<String> tailTailIds = headRelationTailMap.get(tailId).get(nestedRelationId);
                                String nestedRelationKey = getEntityPairKey(relationId, nestedRelationId);

                                for (String tailTailId : tailTailIds) {
                                    mapIncrement(pathCounts, nestedRelationKey);
//                                    System.out.println(nestedRelationKey);

                                    String nestedPairKey = getEntityPairKey(headId, tailTailId);
                                    if (pairRelationMap.containsKey(nestedPairKey)) {
                                        for (String key : pairRelationMap.get(nestedPairKey)) {
                                            mapIncrement(relatedPathCounts, getPathKey(nestedRelationKey, key));
                                        }
                                        float delta = pathResources.get(entityPairKey).get(relationId) / (float) tailTailIds.size();
//                                        System.out.println("\t\t" + nestedPairKey + " " + nestedRelationKey + " " + delta);
                                        nestedMapAdd(pathResources, nestedPairKey, nestedRelationKey, delta);
                                    }
                                }
                            }
                        }
                    }
                }

                // Check all entities as possible tail or 2-hop tails for the current headId
                for (String tailId : headRelationTailMap.keySet()) {
                    String entityPairKey = getEntityPairKey(headId, tailId);
                    if (pathResources.containsKey(entityPairKey)) {
                        path2Writer.write(entityPairKey + "\n");
                        totalPaths += pathResources.get(entityPairKey).size();

                        Map<String, Float> matchingPathResources = new HashMap<>();
                        Map<String, Float> reliablePathResources = new HashMap<>();

                        float sum = 0;
                        for (Map.Entry<String, Float> entry : pathResources.get(entityPairKey).entrySet()) {
                            matchingPathResources.put(entry.getKey(), entry.getValue());
                            sum += entry.getValue();
                        }
                        for (String key : matchingPathResources.keySet()) {
                            matchingPathResources.put(key, matchingPathResources.get(key) / sum);
                            if (matchingPathResources.get(key) > MIN_RESOURCE) {
                                reliablePathResources.put(key, matchingPathResources.get(key));
                            }
                        }

                        path2Writer.write(String.valueOf(reliablePathResources.size()));
                        for (String path : reliablePathResources.keySet()) {
                            reliablePaths.add(path);
                            int length = WHITESPACE_SPLITTER.splitToList(path).size();
                            path2Writer.write(" " + length + " " + path + " " + reliablePathResources.get(path));
                        }
                        path2Writer.write("\n");
                    }
                }

                Log.info("PCRA", String.format("\tStep: %d of %d, totalPaths: %d, "
                        + "time: %dms", countedHeadEntities, headRelationTailMap.size(),
                        totalPaths, System.currentTimeMillis() - startMs));
            }
        }
        Log.info("PCRA", String.format("Executed Path-Constraint Resource "
                        + "Allocation in %dms.", System.currentTimeMillis() - startMs));

        try (BufferedWriter confidenceWriter = FileTools.bufferedWriter(CONFIDENCE_FILE)) {
            for (String path : reliablePaths) {
                List<String> out = new ArrayList<>();
                for (int i = 0; i < relationCount; i++) {
                    String pathKey = getPathKey(path, i);
                    if (pathCounts.containsKey(path) && relatedPathCounts.containsKey(pathKey)) {
                        float confidence = relatedPathCounts.get(pathKey) / (float) pathCounts.get(path);
                        out.add(" " + i + " " + confidence);
                    }
                }
                if (!out.isEmpty()) {
                    confidenceWriter.write("" + WHITESPACE_SPLITTER.splitToList(path).size() + " " + path + "\n");
                    confidenceWriter.write(String.valueOf(out.size()));
                    for (String o : out) {
                        confidenceWriter.write(o);
                    }
                    confidenceWriter.write("\n");
                }
            }
        }

        writePaths(Mode.TRAIN, relationToId, relationCount, pathResources);
        writePaths(Mode.TEST, relationToId, relationCount, pathResources);
    }

    private static Map<String,String> getRelationIds() throws IOException {
        Map<String, String> relationToId = new HashMap<>();

        try (BufferedReader relationReader = FileTools.bufferedReader(KGCompletion.RELATION2ID_FILE)) {
            for (String line = relationReader.readLine(); line != null; line = relationReader.readLine()) {
                Iterator<String> split = WHITESPACE_SPLITTER.split(line).iterator();
                String headId = split.next();
                String tailId = split.next();
                relationToId.put(headId, tailId);
            }
        }
        return relationToId;
    }

    private static int addTrainingRelations(Map<String, Set<String>> pairRelationMap,
            Map<String, Map<String, Set<String>>> headRelationTailMap,
            Map<String, String> relationToId, int relationCount) throws IOException {
        int relationPairs = 0;
        try (BufferedReader trainReader = FileTools.bufferedReader(KGCompletion.TRAIN_FILE)) {
            for (String line = trainReader.readLine(); line != null; line = trainReader.readLine()) {
                Iterator<String> split = WHITESPACE_SPLITTER.split(line).iterator();
                String headId = split.next();
                String tailId = split.next();
                String relation = split.next();
                String relationId = relationToId.get(relation);
                String inverseRelationId = getInverseRelationId(relationId, relationCount);
                relationPairs += 2;

                String tripleKey = getEntityPairKey(headId, tailId);
                pairRelationMap.putIfAbsent(tripleKey, new HashSet<>());
                pairRelationMap.get(tripleKey).add(relationId);

                String reverseTripleKey = getEntityPairKey(tailId, headId);
                pairRelationMap.putIfAbsent(reverseTripleKey, new HashSet<>());
                pairRelationMap.get(reverseTripleKey).add(inverseRelationId);

                headRelationTailMap.putIfAbsent(headId, new HashMap<>());
                headRelationTailMap.get(headId).putIfAbsent(relationId, new HashSet<>());
                headRelationTailMap.get(headId).get(relationId).add(tailId);

                headRelationTailMap.putIfAbsent(tailId, new HashMap<>());
                headRelationTailMap.get(tailId).putIfAbsent(inverseRelationId, new HashSet<>());
                headRelationTailMap.get(tailId).get(inverseRelationId).add(headId);
            }
        }
        return relationPairs;
    }

    /**
     * Add empty pairs for eval (destructively for e1_e2.txt)
     */
    private static int addEvalPairs(Map<String, Set<String>> pairRelationMap) throws IOException {
        int evalPairs = 0;
        for (File evalFile : ImmutableSet.of(KGCompletion.TEST_FILE, KGCompletion.E1_E2_FILE)) {
            try (BufferedReader br = FileTools.bufferedReader(evalFile)) {
                for (String line = br.readLine(); line != null; line = br.readLine()) {
                    Iterator<String> split = WHITESPACE_SPLITTER.split(line).iterator();
                    String headId = split.next();
                    String tailId = split.next();

                    String tripleKey = getEntityPairKey(headId, tailId);
                    String reverseTripleKey = getEntityPairKey(tailId, headId);

                    evalPairs += 2;

                    if (evalFile.equals(KGCompletion.E1_E2_FILE)) {
                        pairRelationMap.put(tripleKey, new HashSet<>());
                        pairRelationMap.put(reverseTripleKey, new HashSet<>());
                    } else {
                        pairRelationMap.putIfAbsent(tripleKey, new HashSet<>());
                        pairRelationMap.putIfAbsent(reverseTripleKey, new HashSet<>());
                    }
                }
            }
        }
        return evalPairs;
    }

    private static void writePaths(Mode mode, Map<String, String> relationToId, int relationCount,
            Map<String, Map<String, Float>> pathResources) throws IOException {
        try (BufferedReader br = FileTools.bufferedReader(mode.getTriplesFile());
                BufferedWriter bw = FileTools.bufferedWriter(mode.getPraFile())) {
            for (String line = br.readLine(); line != null; line = br.readLine()) {
                Iterator<String> split = WHITESPACE_SPLITTER.split(line).iterator();
                String head = split.next();
                String tail = split.next();
                String relationId = relationToId.get(split.next());

                writePaths(bw, head, tail, relationId, pathResources);
                writePaths(bw, tail, head, getInverseRelationId(relationId, relationCount),
                        pathResources);
            }
        }
    }

    // TODO: make triples consistent {head, tail, relationId} -> {headId, tailId, relationId}
    private static void writePaths(BufferedWriter bw, String head, String tail,
            String relationId, Map<String, Map<String, Float>> pathResources) throws IOException {
        bw.write(String.format("%s %s %s\n", head, tail, relationId));

        Map<String, Float> matchingPathResources = new HashMap<>();
        Map<String, Float> reliablePathResources = new HashMap<>();

        String entityPairKey = getEntityPairKey(head, tail);
        if (pathResources.containsKey(entityPairKey)) {
            float sum = 0;
            for (String path : pathResources.get(entityPairKey).keySet()) {
                matchingPathResources.put(path, pathResources.get(entityPairKey).get(path));
                sum += matchingPathResources.get(path);
            }
            for (String path : matchingPathResources.keySet()) {
                matchingPathResources.put(path, matchingPathResources.get(path) / sum);
                if (matchingPathResources.get(path) > MIN_RESOURCE) {
                    reliablePathResources.put(path, matchingPathResources.get(path));
                }
            }
        }

        bw.write(String.valueOf(reliablePathResources.size()));
        for (String path : reliablePathResources.keySet()) {
            bw.write(String.format(" %d %s %f",
                    WHITESPACE_SPLITTER.splitToList(path).size(),
                    path, reliablePathResources.get(path)));
        }
        bw.write("\n");
    }

    private static String getEntityPairKey(String headId, String tailId) {
        return headId + " " + tailId;
    }

    private static String getPathKey(String relation1, String relation2) {
        return relation1 + "->" + relation2;
    }

    private static String getPathKey(String relation1, int relation2) {
        return getPathKey(relation1, String.valueOf(relation2));
    }

    protected static String getInverseRelationId(String relationId, int relationCount) {
        return String.valueOf(Integer.valueOf(relationId) + relationCount);
    }

    private static void nestedMapAdd(Map<String, Map<String, Float>> map, String key1,
            String key2, float delta) {
        map.putIfAbsent(key1, new HashMap<>());
        map.get(key1).putIfAbsent(key2, 0f);
        map.get(key1).computeIfPresent(key2, (k, v) -> v + delta);
    }

    private static void mapIncrement(Map<String, Integer> map, String key) {
        map.putIfAbsent(key, 0);
        map.computeIfPresent(key, (k, v) -> v + 1);
    }

    public static void main(String[] args) throws Exception {
        pathConstraintResourceAllocation();
        PTransEAddTrain add = new PTransEAddTrain();
        add.train();
        //        add.test();
    }
}
